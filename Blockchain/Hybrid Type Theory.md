Hybrid Type Theory: Bridging Formal Systems and Computational Models for Enhanced Verification and Resource Management
Executive Summary
Hybrid Type Theory (HTT) represents an advanced and evolving framework that integrates the rigorous logical foundations of formal systems with the pragmatic computational models essential for modern software and mathematical verification. This report provides an extensive academic exploration of HTT, detailing its foundational principles, mathematical underpinnings, and practical applications. The core of HTT lies in its ability to unify dependent types, which enable precise specification and proof of program correctness, with substructural logics that manage computational resources and state. This integration addresses long-standing challenges in expressing complex properties and ensuring system reliability.
The report delves into the core concepts of Dependent Type Theory (DTT), including the profound Curry-Howard correspondence, which equates types with propositions and terms with proofs, thereby transforming type checking into proof verification. It then elucidates the categorical interpretation of substructural dependent types, demonstrating how novel structures like Left-Fibred Double Categories (LFDCs) provide a unifying metatheory that reconciles seemingly conflicting logical principles. Practical implications are highlighted through the representation of cut admissibility in linear sequent calculus and the application of DTT in formal verification projects such as the CompCert C compiler and the seL4 microkernel. Furthermore, the report examines computational models, including prominent dependently-typed programming languages (e.g., Idris, Agda, Coq), and explores solutions to inherent challenges like the undecidability of type checking through stratified type universes and gradual typing. Emerging applications in areas such as verified artificial intelligence and advanced resource management underscore HTT's potential to revolutionize the design and assurance of complex computational systems.
1. Introduction to Hybrid Type Theory
1.1 Defining Hybrid Type Theory: Bridging Formal Systems and Computational Models
Type theory, at its essence, serves as a formal presentation and academic study of type systems. It has been proposed as a foundational alternative to set theory in mathematics and is extensively employed in computerized proof-writing systems. Within this broad landscape, "Hybrid Type Theory" (HTT) emerges as a sophisticated concept encompassing various approaches that synergistically combine different facets of type theory. Its primary objective is to integrate dependent types with substructural principles or partiality, thereby achieving a richer expressive power and enhanced practical utility.
The "bridging" aspect of HTT is central to its definition. It seeks to unify the rigorous logical foundations traditionally associated with formal systems with the pragmatic requirements and computational characteristics inherent in programming languages. This endeavor is driven by the recognition that while formal systems excel in logical precision, they often lack the mechanisms to effectively model dynamic computational properties such as resource consumption or state changes. Conversely, computational models, while practical, frequently lack the formal guarantees of correctness provided by deep logical frameworks. HTT aims to construct a coherent framework that simultaneously addresses both concerns.
The emergence of hybrid type theories is a direct response to the expressive limitations observed in individual formalisms. Traditional dependent type theories are exceptionally powerful for internalizing mathematical reasoning and constructing proofs and programs by ensuring that programs are correct by construction. Concurrently, substructural type systems, such as linear logic, are highly effective at capturing notions of state and managing computational resources, reflecting the fundamental insight that "truth is ephemeral" or resources are consumed. However, a critical challenge has been observed where "substructurality must end where dependency begins". This statement articulates a fundamental difficulty in combining these powerful yet seemingly antithetical expressive capabilities within a single, coherent system. The underlying issue is that the fine-grained control over resource usage (substructurality) often conflicts with the free access and manipulation of values required for dependent types.
Hybrid Type Theory thus represents a sophisticated synthesis rather than a mere additive combination. It is designed to resolve this inherent expressive limitation, aiming to overcome traditional boundaries where the strengths of one system previously precluded the full integration of another. This pursuit leads to a more comprehensive and practically applicable framework capable of simultaneously modeling both the logical correctness derived from deep mathematical reasoning and the precise computational resource management essential for practical applications. Such an integrated approach addresses a perceived "expressive ceiling" in individual formalisms, paving the way for systems that are both logically sound and computationally efficient.
1.2 Historical Context and Motivation
The origins of type theory are deeply rooted in the early 20th century, primarily motivated by the need to circumvent paradoxes that plagued naive set theory and formal logic, most notably Russell's paradox. Bertrand Russell proposed a hierarchy of types, assigning each mathematical entity to a specific type, ensuring that entities of a given type were built exclusively from subtypes. This stratification prevented self-referential definitions that led to contradictions, offering a consistent foundation for mathematics. Alonzo Church further advanced this foundational work with his simply typed lambda calculus, which successfully avoided the Kleene–Rosser paradox that affected untyped lambda calculus, demonstrating its potential as a foundation for higher-order logic.
A pivotal development in type theory was Per Martin-Löf's Intuitionistic Type Theory (MLTT). Proposed as a foundation for constructive mathematics, MLTT introduced the profound "propositions as types" and "proofs as programs" principles, establishing a deep connection between logic and computation. This framework promised to internalize mathematical reasoning directly into systems for constructing both proofs and programs. Simultaneously, Jean-Yves Girard's linear logic introduced a radical perspective where "truth is ephemeral," providing a formal system to capture notions of state and resources explicitly. This contrasted with traditional logic, where assumptions could be used arbitrarily (weakening and contraction rules).
The motivation for hybrid approaches stems from the inherent limitations of these individual powerful formalisms when applied to real-world computational problems. While Martin-Löf's dependent types excelled at ensuring logical correctness and program behavior, they did not inherently account for resource consumption. Conversely, linear logic provided resource management but lacked the rich dependent types necessary for complex data structures and proofs. Prior work often struggled to combine these strengths into a single, coherent system, as "substructurality must end where dependency begins". This indicated a fundamental incompatibility: the strict resource accounting of substructural types seemed to conflict with the unconstrained access to values often implied by dependent types.
This historical trajectory illustrates type theory's maturation from a purely theoretical logical tool into a sophisticated engineering discipline. The initial purpose of type theory was fundamentally correctness-focused, aiming to build consistent logical systems to avoid paradoxes. Martin-Löf's work broadened this scope, expanding type theory's utility to constructive mathematics and the formal semantics of programming languages, marking a significant shift towards practical computational relevance. The introduction of resource awareness through linear logic further added a critical dimension concerned with the computational cost and statefulness of operations. The explicit acknowledgement that "prior work... has struggled to combine these all in one and the same system" and that "substructurality must end where dependency begins" reveals a new, more ambitious driving force for hybrid type theories. The motivation is no longer solely about foundational consistency or individual application domains, but about achieving a unified and integrated expressive power. This integration allows for simultaneous reasoning about program correctness (via dependent types) and resource efficiency/state (via substructural types) within a single, coherent framework. The "hybrid" nature of current research signifies the cutting edge in designing systems that are simultaneously logically sound, mathematically rigorous, and practically capable of modeling complex computational phenomena, including resource constraints and state.
2. Foundational Concepts: Dependent Type Theory (DTT)
2.1 Core Principles of Dependent Types
Dependent Type Theory (DTT) is a branch of type theory where the definition of a type can depend on a value. This fundamental characteristic allows for the creation of significantly more precise and expressive type definitions compared to traditional type systems. For instance, in a conventional programming language, a list of integers might simply have the type List Int. In a dependently typed language, one can define a type Vect n a that represents a vector of length n containing elements of type a, where n is a natural number (a value). This ensures that operations on such vectors, like accessing an element by index, are type-safe and prevent common runtime errors such as out-of-bounds access.
Dependent types enable the encoding of complex properties and relationships directly into the type system, leading to the development of more robust and maintainable software. For example, a function designed to operate only on non-empty lists could be given a type that explicitly reflects this constraint, and the type checker would statically verify that only non-empty lists are passed to it.
Two fundamental constructors in DTT are dependent function types (Π-types) and dependent pair types (Σ-types).
Dependent Function Types (Π-types): These generalize ordinary function types where the return type can depend on the value of the argument. For example, a function f : (n : Nat) -> Vect n Int takes a natural number n and returns a vector of integers whose length is exactly n. This corresponds to universal quantification ("for all n...").
Dependent Pair Types (Σ-types): These generalize ordinary product types (pairs) where the type of the second component depends on the value of the first component. For example, (n : Nat) × Vect n Int represents a pair consisting of a natural number n and a vector of integers of precisely that length n. This corresponds to existential quantification ("there exists an n such that...").
These constructs allow for expressing intricate logical propositions and data invariants directly within the type system, transforming type systems into powerful specification languages.
2.2 The Curry-Howard Correspondence: Propositions as Types, Proofs as Programs
The Curry-Howard correspondence is a profound and fundamental principle that establishes a deep isomorphism between logical systems and type theories. This correspondence asserts that a type can be interpreted as a logical proposition, and a term (or program) that inhabits that type can be interpreted as a proof of that proposition. This is not merely an analogy; it reveals a structural identity where the construction of a program is equivalent to the construction of a proof.
This principle fundamentally blurs the traditional distinction between mathematical logic and computer programming. Historically, these disciplines were often treated as separate, with logic focusing on abstract truth and inference rules, and programming dealing with concrete execution and computation. The Curry-Howard correspondence bridges this divide. The interpretation of "types as propositions" means that the very structure and well-formedness of data types within a program can be understood as logical statements. For example, a type A -> B corresponds to the proposition A implies B, and a pair type A * B corresponds to A and B. Concurrently, "terms as proofs" implies that the act of constructing a program (a term) is equivalent to constructing a proof of the proposition (type) it inhabits.
This causal relationship means that the computational behavior of terms—their ability to reduce to canonical forms, as described in Martin-Löf Type Theory —directly realizes the logical validity of the propositions they represent. Consequently, if a program successfully type-checks, it is provably correct by construction, thereby eliminating an entire class of potential logical and runtime errors. This transforms type checking into a rigorous method of proof checking. For instance, a constructive proof that "there are arbitrarily large primes" can be represented as a term of a specific dependent type. This term, when evaluated, would yield a program that, given any number, produces a larger prime along with explicit proofs (sub-terms) demonstrating that it is indeed larger and prime.
This unification leads to a powerful new paradigm where software development can be viewed as a form of constructive mathematics, and conversely, mathematical proofs can be directly executable as programs. It elevates the role of type systems from a mere mechanism for error prevention to a foundational method for ensuring inherent correctness by design. This is a driving force behind the formal verification revolution, enabling the creation of highly reliable software and the machine-checking of complex mathematical theorems.
2.3 Formal Judgments in Martin-Löf Type Theory
In Martin-Löf Type Theory (MLTT), judgments are fundamental acts of knowledge, conceptually distinct from propositions themselves. Propositions are objects of the theory about which judgments can be made. The meaning of these judgments is intrinsically tied to the computation of types and terms to their canonical forms, which are terms whose outermost structure is a constructor.
There are four basic forms of categorical judgments, which are made without any assumptions or free variables:
⊢ A type: This states that A is a well-formed type. Its meaning is established if A computes to a canonical type (e.g., a natural number type, a function type).
⊢ a : A: This states that a has type A. Under the propositions-as-types interpretation, this means a is a proof-object for proposition A. Its meaning is established if a computes to a canonical term of the canonical type of A.
⊢ A = A': This states that A and A' are equal types. This holds if A and A' compute to the same canonical type.
⊢ a = a' : A: This states that a and a' are equal elements of type A. This holds if a and a' compute to equal canonical terms of type A.
Judgments are more generally hypothetical, meaning they are made within a context Γ. A context Γ is a list of variables paired with their types, such as x₁ : A₁,..., xₙ : Aₙ. Types within a context can depend on variables introduced earlier in the list. The four forms of hypothetical judgments mirror their categorical counterparts:
Γ ⊢ A type: A is a well-formed type in the context Γ.
Γ ⊢ a : A: a has type A in context Γ.
Γ ⊢ A = A': A and A' are equal types in the context Γ.
Γ ⊢ a = a' : A: a and a' are equal elements of type A in the context Γ.
Additionally, the judgment Γ context (or Γ ⊢.) explicitly states that Γ is a well-formed context, and Γ = Γ' means Γ and Γ' are equal contexts. The meaning of hypothetical judgments is reduced to categorical judgments by substituting closed terms for free variables in the context.
The distinction between judgmental equality (e.g., Γ ⊢ a = a' : A) and propositional equality (e.g., Id(A, a, b) or a ≡ b) is crucial in MLTT. Judgmental equality signifies a definitional equivalence established by the system's computation rules, often by checking if terms reduce to the same canonical form. This form of equality is implicit and computationally fundamental; if terms are judgmentally equal, they are interchangeable by definition within the system without requiring explicit proof. In contrast, propositional equality is represented by an identity type, which is a type that is inhabited by a proof term if two elements are equal. This requires an explicit proof object to demonstrate the equality.
This duality is critical for formal verification. Definitional equality is a built-in computational property, while propositional equality is a logical statement requiring derivation. The choice between intensional type theories (where judgmental equality is strict and does not necessarily imply propositional equality) and extensional type theories (where propositional equality implies judgmental equality) hinges on this fundamental difference. This nuanced approach to equality allows for a sophisticated framework for formal reasoning, where some equivalences are handled automatically by the computational rules, while others require explicit logical construction. This is particularly relevant in advanced areas like Homotopy Type Theory, where identity types acquire a rich, higher-dimensional structure, reflecting deeper mathematical equivalences.
Here is a table summarizing the core judgments in Martin-Löf Type Theory:
Table: Core Judgments in Martin-Löf Type Theory
Judgment Form
Meaning/Explanation
Canonical Forms/Interpretation
Source References
\Gamma \text{ context} or \Gamma \vdash.
\Gamma is a well-formed context.
A list of variable declarations x₁:A₁,..., xₙ:Aₙ where each Aᵢ is a well-formed type in x₁:A₁,..., xᵢ₋₁:Aᵢ₋₁.


\Gamma \vdash A \text{ type}
A is a well-formed type in context \Gamma.
A computes to a canonical type (e.g., \mathbb{N}, A \to B, \Pi x:B. C, \Sigma x:B. C, \text{U} (universe)).


\Gamma \vdash a : A
a has type A in context \Gamma.
a computes to a canonical term of the canonical type of A. (e.g., 0, \text{S}(b), \lambda x.c, \langle b,c \rangle).


\Gamma \vdash A = A'
A and A' are equal types in context \Gamma.
A and A' compute to the same canonical type.


\Gamma \vdash a = a' : A
a and a' are equal elements of type A in context \Gamma.
a and a' compute to equal canonical terms of type A.



3. Bridging Formal Systems: Substructural and Categorical Foundations
3.1 Substructural Type Systems and Linear Logic
Substructural type systems are a class of formal systems that explicitly restrict or modify the traditional structural rules of logic, namely weakening (the ability to discard assumptions) and contraction (the ability to duplicate assumptions). In classical and intuitionistic logic, premises can be used an arbitrary number of times or discarded if not needed. Substructural logics challenge this assumption, treating propositions and their corresponding proofs as resources that must be managed.
Jean-Yves Girard's linear logic is a prominent example of a substructural system, where the central tenet is that resources are "used exactly once". This "use-exactly-once" policy for runtime behavior is a defining characteristic. The motivation behind linear logic and other substructural systems is to capture notions crucial for modeling computational systems accurately, such as state, resource consumption, and the ephemeral nature of truth. For instance, in a linear type system, a variable representing a file handle might be consumed after a read operation, preventing accidental double-reads or forgotten closures.
The integration of dependent types with substructural systems presents a significant challenge. Prior work often found that "substructurality must end where dependency begins". This statement highlights a deep tension between resource management and information flow in type systems. Dependent types inherently allow for types to refer to values, implying a form of "free access" or "contemplation" of information. If a type's definition depends on a value (e.g., a resource), and that value is subject to linear consumption or duplication, maintaining consistency becomes problematic. The type's very definition could become unstable or invalid if its underlying value is no longer available or its multiplicity has changed due to linear operations.
This tension arises because the act of "depending on" or "inspecting" a value in a type is traditionally considered non-consuming, often referred to as "contemplation". However, if the value itself is a resource that must be managed, then this "contemplation" might implicitly violate the strict resource constraints of the linear logic. The challenge is to reconcile this informational dependency with the resource-aware operational semantics. Hybrid Type Theory aims to resolve this by developing sophisticated mechanisms that allow values to appear in types without compromising the rigorous resource constraints of the underlying substructural logic. This represents a significant advancement in building type systems that can simultaneously reason about both logical correctness and fine-grained computational resource management in a unified framework.
3.2 Categorical Interpretation of Substructural Dependent Types
Category theory provides a powerful abstract framework for modeling type theories, offering a means to abstract mathematical structure and precisely define what constitutes a "model" of a type theory. This approach allows for the study of complex meta-theoretic properties and can help "legitimize" new flavors of type theory by providing concrete semantics.
A novel approach to integrating dependent types into substructural type systems is presented through the use of Left-Fibred Double Categories (LFDCs). This framework generalizes the traditional categorical semantics of dependent types, particularly those based on comprehension categories, to accommodate substructural behavior.
Mathematical Definitions and Properties of Categorical Models:
The categorical interpretation begins by revisiting the ordinary semantics of dependent type theory, focusing on Comprehension Categories. A comprehension category, which models dependent type theory , comprises:
A category \mathfrak{C} of contexts. Objects \Gamma, \Delta represent contexts, and morphisms f: \Gamma \to \Delta \in \mathfrak{C} are viewed as substitutions.
For each context \Gamma \in \mathfrak{C}, a category \mathfrak{T}(\Gamma) of types. Objects A are types dependent upon \Gamma. Crucially, morphisms t: S \to T \in \mathfrak{T}(\Gamma) are terms of type T in context \Gamma, parameterized by S. This "extra parameterization" of terms as morphisms is vital for generalizing to substructural dependent types and separating "type-level" and "term-level" aspects of contexts.
For each substitution \sigma: \Gamma \to \Delta \in \mathfrak{C}, a functor \sigma^*: \mathfrak{T}(\Delta) \to \mathfrak{T}(\Gamma) representing the application of the substitution.
For each \Gamma \in \mathfrak{C}, a context extension functor \Gamma \bullet: \mathfrak{T}(\Gamma) \to \mathfrak{C} mapping each type A to the context \Gamma \bullet (A), which is intuitively \Gamma extended with a fresh variable of type A.
Natural transformations \sigma \bullet and \pi_\Gamma that ensure compatibility and define projection maps. These projection maps \pi_\Gamma induce "term-level weakening," allowing the discarding of the extended part of a context, a property often restricted in substructural settings.
To obtain models of substructural dependent type theory, the framework generalizes to LFDCs. An LFDC consists of the same data as items 1-5 of a comprehension category, along with:
A dependent pair type functor \oplus: \mathfrak{T}(\Gamma \bullet (A)) \to \mathfrak{T}(\Gamma).
Natural transformations and isomorphisms ensuring coherence and compatibility of these structures.
A unit type object 1_\Gamma \in \mathfrak{T}(\Gamma).
Key Properties of LFDCs:
Any comprehension category with strong sums and strong unit types is inherently an LFDC, demonstrating that models of ordinary dependent type theory are naturally encompassed by LFDCs.
The LFDC concept also extends to categorical models of linear/ordered logic, specifically monoidal categories. In this case, a monoidal category \mathcal{M} is interpreted as both the category of contexts and types, with context extension and dependent pair types interpreted as monoidal products.
A critical feature is that LFDCs can exhibit both non-trivial type-dependency and substructural behavior, unlike prior approaches that confined dependency to an intuitionistic layer. For example, the category of linearly ordered sets, where context extension and dependent pair types are interpreted as the lexicographic sum of linear orders, is non-symmetric and thus substructural.
The definition of an LFDC is further augmented with notions of type-level weakening, term-level function types, type-level function types, and product types. Each of these is defined through specific functors, adjoints, and natural isomorphisms that ensure their compatibility with the LFDC structure and substitution. For the purpose of type theory, a "strict LFDC" is introduced, where substitution and type-level weakening strictly preserve identities and composites, and Beck-Chevalley isomorphisms are identities. Syntactic categories of dependent type theory are examples of strict LFDCs.
The categorical framework, particularly the introduction of LFDCs, serves as a unifying metatheory for hybrid systems. The fundamental difficulty in integrating dependent types with substructural logic stems from their seemingly conflicting structural rules, often leading to approaches where dependency is confined to a separate, intuitionistic layer. The LFDC framework is designed to overcome this limitation. By abstracting the universal property of context extension (a hallmark of intuitionistic DTT) to a more general "unital and associative structure," LFDCs can simultaneously model both the rich expressiveness of dependent types and the precise resource management of substructural behavior.
The "extra parameterization of terms as morphisms" in the category of types \mathfrak{T}(\Gamma) is a key innovation. It allows for a finer-grained control over how terms interact with contexts, enabling the crucial separation of "type-level" inspection (which does not consume resources) from "term-level" operations (which do). This separation is essential for maintaining linearity while allowing dependency. This categorical approach provides a powerful metatheoretical lens for understanding and designing hybrid type theories. It offers a mathematically precise and abstract way to unify seemingly disparate logical systems, demonstrating that their underlying structures can be seen as specific instances or variations within a more general categorical framework. This is a significant step towards a unified theory of types that can encompass a wider range of computational phenomena, including both logical correctness and resource-aware behavior.
3.3 Application: Representing Cut Admissibility for Linear Sequent Calculus
The practical utility of the categorical interpretation of substructural dependent types is powerfully demonstrated through its application in representing cut admissibility for linear sequent calculus. The Logic of Left-Fibred Double Categories with Symmetry (LLFDCΣ) is particularly well-suited as a logical framework for the metatheory of linear logic, specifically for proving and representing cut admissibility.
LLFDCΣ is defined as a substructuralization of intuitionistic Dependent Type Theory, incorporating exchange rules but explicitly excluding term-level weakening and contraction as structural rules. This careful selection of structural rules is crucial for maintaining the precise resource accounting characteristic of linear logic.
Formal Derivations and Proof Sketches:
The paper demonstrates the encoding of linear sequent calculus using Higher-Order Abstract Syntax (HOAS) within LLFDCΣ. This involves:
Postulating atomic types for propositions (Prop) and type families for antecedents (Ante) and consequents (Conse).
A derivation Γ ⊢ A in intuitionistic linear sequent calculus corresponds to a closed term of type ∀x₁,...,xₙ, A: Prop. (Ante(x₁) ⊗... ⊗ Ante(xₙ)) ⊸ Conse(A). Here, ⊗ denotes the multiplicative conjunction and ⊸ denotes linear implication.
Constructors for linear logic connectives (e.g., linear implication ⊸) and rules (e.g., ⊸R, ⊸L, id, cut) are included as axioms or constants in the signature of LLFDCΣ.
To represent cut-free proofs, a type family CF is introduced. For example, cfId would represent an identity axiom in a cut-free derivation, and cf⊸R, cf⊸L would represent linear implication right and left rules, respectively, when applied to proofs constructed from identity or other cut-free rules.
The core challenge in metatheory is demonstrating cut admissibility, which states that any proof using the cut rule can be transformed into an equivalent proof that does not use cut. To handle derivations whose outermost constructor is cut, a relation CutStep is introduced to capture single-step cut reduction. This relation leverages the product type former × to allow forming a derivation of type Conse(A) in the same context as the inputs to cut. The inputs to cut are represented with a type like Conse(A) ⊗ (Ante(A) ⊸ Conse(B)), which requires the context to be split accordingly for resource accounting.
The crucial aspect of this construction is that it forces the parameters of CutStep to be used linearly. This means that every derivation occurring as a parameter in the inputs to a cut must be used in the same quantity in the output of the cut reduction. This linearity constraint directly addresses and solves the issues highlighted by Jason Reed regarding spurious cut elimination rules in other linear logical frameworks. Reed's examples of ill-typed cut reduction axioms, where variables were used twice or not at all in term-level positions, are shown to be ill-typed in LLFDCΣ due to the enforced linearity constraints. This demonstrates that LLFDCΣ provides a robust framework where such problematic derivations are inherently prevented by the type system itself.
The success of LLFDCΣ in representing cut admissibility implies that for every derivation using cut, there is a corresponding cut-free proof, and that the cut reduction process terminates. This provides a strong metatheoretic guarantee for linear logic within a dependently typed framework.
4. Computational Models and Practical Applications
4.1 Dependent Type Programming Languages
The theoretical advancements in Dependent Type Theory (DTT) have significantly influenced the design of modern programming languages, enabling developers to write code with stronger correctness guarantees. These languages leverage dependent types to embed complex properties and invariants directly into the type system, which are then verified at compile-time.
Prominent dependently-typed programming languages include:
Idris: A general-purpose functional programming language designed to support DTT. Idris allows types to depend on values, making it possible to express intricate invariants within the type system. For example, a function that takes a list and returns its head can be typed such that it only accepts non-empty lists, preventing runtime errors.
Agda: Closely related to Idris, Agda is another purely functional language based on Martin-Löf's type theory, widely used as a proof assistant. Its strong type system allows for the formalization of mathematical theorems and the execution of proofs as algorithms. Agda uses dependently typed pattern matching for defining functions and inductive proofs.
Coq: A popular proof assistant based on the Calculus of Inductive Constructions, a powerful dependent type theory. Coq is extensively used for formal verification of software and mathematical proofs, enabling machine-checkable and human-readable proofs.
Lean: Developed at Microsoft Research, Lean is another proof assistant based on dependent type theory, known for its performance and usability in formalizing mathematics.
F*: A functional-first language that incorporates dependent types for formal verification and programming language research, allowing for strong correctness guarantees in practical software.
ATS (Applied Type System): A programming language that supports dependent types and linear types, allowing for fine-grained control over memory and resource management.
NuPRL (Cornell University): A proof development system based on Martin-Löf intuitionistic type theory, used for computer-mediated analysis and proofs of formal mathematical statements, and software verification.
Matita: An interactive theorem prover based on a variant of the Calculus of (Co)Inductive Constructions, compatible with Coq to some extent.
Twelf: A logic programming language that serves as a meta-logical framework for specifying and reasoning about deductive systems, using dependent types to encode judgments and inference rules.
SPARK (Ada): A programming language and formal verification toolset based on Ada, which leverages features like type safety and contracts to prove the absence of runtime errors and conformance to specifications.
Code Examples:
To illustrate the practical application of dependent types, consider the common example of a length-indexed vector. This data structure ensures that its length is part of its type, preventing out-of-bounds errors at compile time.
Idris Example: Length-indexed Vector and Safe Head Function In Idris, a vector Vect n a has a type parameter n (a natural number representing length) and a (the element type).
-- Definition of Natural Numbers (Nat)
data Nat = Z | S Nat

-- Definition of a length-indexed Vector
data Vect : Nat -> Type -> Type where
  Nil  : Vect Z a
  (::) : a -> Vect n a -> Vect (S n) a

-- Example vectors
v1 : Vect 3 Int
v1 = 1 :: 2 :: 3 :: Nil

v0 : Vect 0 String
v0 = Nil

-- A safe head function for non-empty vectors
-- The type ensures the input vector has at least one element (S n)
head : {n : Nat} -> {a : Type} -> Vect (S n) a -> a
head (x :: xs) = x

-- This would type-check:
-- result1 : Int
-- result1 = head v1  -- result1 is 1

-- This would cause a compile-time type error:
-- head v0 -- Error: Cannot match Vect 0 with Vect (S n)


The head function's type Vect (S n) a -> a explicitly states that it only accepts vectors whose length is the successor of some natural number n, meaning it must be non-empty. This prevents calling head on an empty vector, a common source of runtime errors in languages without dependent types.
Agda Example: Natural Number Addition with Proof of Identity Agda, being an extension of Martin-Löf's type theory, allows defining functions and proving properties about them directly within the language.
-- Definition of Natural Numbers (ℕ)
data ℕ : Set where
  zero : ℕ
  suc  : ℕ → ℕ

-- Addition of natural numbers
_╱ +_ : ℕ → ℕ → ℕ
zero  + n = n
(suc m) + n = suc (m + n)

-- Proof that adding zero to any natural number n results in n
-- This is a theorem expressed as a dependent type:
-- For all n (n : ℕ), n + zero equals n (n ≡ n + zero)
plus-zero-right-identity : (n : ℕ) → n ≡ n + zero
plus-zero-right-identity zero    = refl  -- Base case: 0 ≡ 0 + zero, which is 0 ≡ 0. Proven by reflexivity (refl).
plus-zero-right-identity (suc n) = cong suc (plus-zero-right-identity n)
  -- Inductive step: (suc n) ≡ (suc n) + zero
  -- By definition, (suc n) + zero reduces to suc (n + zero).
  -- We need to show suc n ≡ suc (n + zero).
  -- The inductive hypothesis is n ≡ n + zero.
  -- 'cong suc' applies the successor function to both sides of the inductive hypothesis,
  -- transforming (n ≡ n + zero) into (suc n ≡ suc (n + zero)).
  -- Since suc (n + zero) is definitionally equal to (suc n) + zero, the proof holds.


The plus-zero-right-identity function is a proof that n + zero is judgmentally equal to n. The refl constructor is a proof of propositional equality x ≡ x. cong (congruence) is a standard lemma stating that equality is preserved under function application. This example demonstrates how programs (functions) are proofs (terms of equality types).
Coq Example: Length-indexed List (Vector) Coq uses similar concepts for defining data types with dependent types.
(* Define Natural Numbers *)
Inductive nat : Type :=

| O : nat
| S : nat -> nat.

(* Define a length-indexed list (Vector) *)
Inductive vector (A : Type) : nat -> Type :=

| Vnil : vector A O
| Vcons : forall n, A -> vector A n -> vector A (S n).

(* Example vectors *)
Definition my_vec_int : vector nat (S (S O)) :=
  Vcons (S O) (Vcons O Vnil). (* This is [1; 0] *)

Definition my_vec_bool : vector bool (S O) :=
  Vcons true Vnil. (* This is [true] *)

(* A safe head function for non-empty vectors *)
Definition vhead {A : Type} {n : nat} (v : vector A (S n)) : A :=
  match v with

| Vcons _ x _ => x
  end.

(* This would type-check: *)
(* Compute (vhead my_vec_int). *) (* Result: S O : nat *)

(* This would cause a compile-time type error if 'Vnil' was passed: *)
(* Definition empty_vec : vector nat O := Vnil. *)
(* Compute (vhead empty_vec). *) (* Error: The term "empty_vec" has type "vector nat O"
                                    while it is expected to have type "vector?A (S?n)". *)


This Coq example is analogous to the Idris vector example, demonstrating how the type system enforces correctness properties, such as ensuring that the vhead function is only applied to non-empty vectors. These languages provide powerful tools for developing highly reliable software by shifting error detection from runtime to compile time.
4.2 Formal Verification and Proof Assistants
Formal verification is the process of using rigorous mathematical techniques to prove the correctness of a system or a piece of software. It is a crucial step in ensuring the reliability and safety of complex systems, particularly in critical domains such as aerospace, finance, and healthcare. This process involves creating a formal mathematical model of the system and then using mathematical proofs to verify that it meets its specifications.
Dependent Type Theory plays a significant role in formal verification due to its ability to encode complex mathematical structures and proofs directly within the type system. As established by the Curry-Howard correspondence, a type is a proposition, and a term of that type is a proof of that proposition. This enables type checking to be used as a means of proof checking, allowing for the creation of formal proofs that are both machine-checkable and human-readable.
Proof assistants are interactive software tools that aid in the development and verification of formal proofs. They provide a formal language for expressing mathematical statements and a set of rules for constructing proofs. Prominent proof assistants that leverage DTT include:
Coq: A widely used proof assistant for formalizing complex mathematical proofs and verifying software.
Lean: Another powerful proof assistant, increasingly popular for formalizing large bodies of mathematics.
HOL Light: A proof assistant based on higher-order logic, which has been used for significant verification projects.
Isabelle/HOL: A generic proof assistant framework that supports various logics, including higher-order logic, and is used for formal verification.
Several notable case studies demonstrate the effectiveness of DTT in formal verification:
Kepler Conjecture: The formal verification of the Kepler conjecture, a problem in geometry concerning the most efficient way to pack spheres, was achieved using the HOL Light proof assistant. This involved a combination of mathematical insights and computer-assisted proof checking.
CompCert C Compiler: The correctness of the CompCert C compiler, a highly optimizing C compiler, was formally verified using the Coq proof assistant. This landmark achievement demonstrated the feasibility of verifying industrial-strength software components.
seL4 Microkernel: The seL4 microkernel is a formally verified operating system kernel, developed using type theory and formal verification techniques. This project provides strong guarantees about the absence of software bugs in a critical system component.
Mathematical Theorems: DTT-based proof assistants like Coq and Lean have been instrumental in the formal verification of various complex mathematical theorems, including the Four Color Theorem and the Odd Order Theorem. These efforts reduce the likelihood of errors in intricate mathematical proofs and contribute to the reliability of mathematical knowledge.
The application of DTT in formal verification provides stronger guarantees about program correctness, improves code reuse by enabling generic and reusable code parameterized by types and properties, and allows for formal proofs that are both machine-checkable and human-readable. This capability is transforming how critical software and mathematical theories are developed and validated.
4.3 Addressing Undecidability in Type Checking
While Dependent Type Theory offers immense expressive power and strong guarantees, it introduces significant challenges, particularly concerning the decidability of type checking. In general, type checking for dependent type systems is undecidable. This undecidability stems from the fact that type checking in DTT often requires evaluating terms embedded within types. If these terms can represent arbitrary programs, then deciding type equality might involve solving the Halting Problem, which is known to be undecidable. Other related undecidable problems include the Totality Problem (determining if a program halts for all inputs) and the Equivalence Problem (determining if two programs compute the same function).
To mitigate this fundamental challenge, several solutions and approaches have been developed:
Stratified Type Universes: This approach addresses self-reference paradoxes and undecidability by introducing a hierarchy of types or "universes."
Ramified Type Theory (RTT): Developed by Bertrand Russell, RTT stratifies statements into a hierarchy based on their complexity and the types of objects they refer to, using both types and orders. This ensures that a proposition can never refer to itself directly, preventing paradoxes like Russell's paradox.
Stratified Type Theory (StraTT): A more modern approach, StraTT, rather than stratifying universes by levels, stratifies typing judgments themselves by levels and restricts the domain of dependent functions to strictly lower levels. This design, inspired by Leivant's Stratified System F, aims to enforce consistency even with the type-in-type axiom. The strict level ordering for dependent functions prevents a type from classifying itself or being used in a way that creates a circular dependency, thereby preventing impredicativity and paradoxes. SubStraTT, a subsystem of StraTT, has been formally proven logically consistent in Agda.
Coq's Universe Polymorphism: Coq, based on the predicative Calculus of Inductive Constructions (pCIC), uses an infinite hierarchy of predicative Typei universes and an impredicative Prop sort. A cumulativity relation (Typei <: Typej if i <= j) ensures consistency. Universe polymorphism extends this by allowing generic definitions over universes that can be reused at different levels, while still generating constraints to ensure consistency. This system prevents paradoxes by ensuring that types are organized into levels, preventing self-referential definitions.
Gradual Typing: This approach aims to make dependently-typed languages more flexible and easier to adopt by allowing a smooth transition between untyped and fully dependently-typed code.
GDTL (Gradual Dependent Type Language): GDTL addresses the challenges of non-termination and runtime errors in gradually-typed code by distinguishing between compile-time normalization (approximate but total) and runtime execution (exact but potentially failing or diverging). It satisfies static and dynamic gradual guarantees, meaning reducing type precision preserves typedness, and altering type precision does not change program behavior outside of dynamic type failures.
Partial Gradual Dependent Type Theory: This is an attempt to challenge the "no-go" result (which states one can only choose two from normalization, graduality, and semantic conservativity) by imposing restrictions on imprecise types, preventing the embedding of untyped lambda calculus and simplifying runtime checks.
Normalization by Evaluation (NBE): This technique is used to determine when two types are the same, which is crucial for type checking in DTT. When types can contain programs, NBE evaluates these programs to their normal forms to check for equality. While type checking in DTT depends on normalization, and its complexity can be extremely high , NBE provides a principled way to perform this check.
These solutions represent ongoing research efforts to balance the expressive power of dependent types with the practical requirement of decidable and efficient type checking, making these advanced type systems more usable in real-world software development and formal verification.
4.4 Resource-Awareness and Linear Dependent Types in Practice
The integration of resource-awareness into dependent type systems is a critical area of research, primarily addressed through linear dependent types. These systems extend traditional type theory to reason about program resource usage in a compositional manner, ensuring properties like "use-exactly-once" policies for runtime behavior.
Linear indexed type systems employ a type-level index language to describe resources. By adding dependent types, these systems allow resource usage to depend on runtime information, such as the size of a data structure or the sensitivity of functions. This significantly enriches the analysis, enabling more precise resource annotations for higher-order functions that are difficult to type in other systems.
For instance, in a novel dependent linear type theory, the multiplicity of a variable (i.e., the number of times it can be used in a program) can itself depend on other variables. This is achieved by embedding linear logic into dependent type theory and specifying how the embedded logic interacts with the host theory. A standard natural numbers type can then be used to obtain a quantitative typing system with dependent multiplicities. This approach is inspired by the Dialectica translation, which provides a method to transform an intuitionistic calculus into a linear one. By adding function types and inductive types, a fully-fledged dependent linear type theory is obtained where the multiplicity of some input variable can depend on another variable. This system can be added to existing dependently typed languages, as demonstrated by an experimental implementation in Agda.
The formalisms often stipulate the existence of new types, such as a type of Supplies and a type of Productions, equipped with the structural rules of linear logic. The key is that the theory adds more structure to an existing dependent type theory rather than changing its fundamental structural rules. This allows for precise resource annotations without caring about the usage of variables in types, only in programs, leading to a rich language for specifying program behavior.
Despite the expressive power, type checking and inference in linear dependent type systems become more complex. For example, in DFuzz, a system with linear dependent types, type checking constraints may involve general polynomials rather than just constants, and type checking for DFuzz has been shown to be undecidable. Solutions often involve constraint-solving procedures, sometimes compiling constraints to first-order logic for standard solvers, or employing constraint relaxation for approximation. The challenge lies in ensuring that the inferred sensitivities are the best possible while maintaining decidability.
4.5 Homotopy Type Theory (HoTT) and its Computational Implications
Homotopy Type Theory (HoTT) represents a significant paradigm shift in the foundations of mathematics and type theory. It reinterprets Martin-Löf's identity types, traditionally understood as simple propositions of equality, as "paths" or "identifications" in a topological sense. In HoTT, equality is not merely a property but data, meaning there can be multiple distinct proofs (paths) between two equal terms. This leads to the explicit rejection of the Unique Identity Proofs (UIP) axiom, which states that any two proofs of the same equality are themselves equal.
The rejection of UIP has profound implications. In type theory without UIP, internal equalities (like associativity) behave more like isomorphisms in higher categories (e.g., bicategories or \infty-categories) than simple laws in a 1-category. This means that higher equalities of any level can be non-trivial data, reflecting a richer, higher-dimensional structure of mathematical objects.
This reinterpretation impacts formal verification by offering a more expressive language for modeling and proving properties. HoTT allows for more precise and rigorous proofs, potentially reducing errors and supporting higher-level abstractions. For example, it enables a more nuanced understanding of "extensional equality," where two objects are considered interchangeable if they behave the same way, regardless of their internal construction. This is crucial for reusability and modularity in software systems, as a component should be replaceable by another that behaves identically.
However, HoTT also presents challenges for computational models and formal verification:
Limited Tool Support: While proof assistants like Coq and Agda have some support for HoTT concepts, comprehensive tool support is still developing.
Complexity: HoTT is a relatively new and complex field, requiring significant expertise. Scaling HoTT-based formal verification to large systems remains challenging.
Computational Justification: Many presentations of HoTT involve axioms that currently lack direct computational justification, making it difficult to build programming languages or verification systems directly based on them.
To address the complexities of internalizing higher-dimensional structures in HoTT, particularly in the absence of UIP, the concept of \infty-categories with families (\infty-CwF's) has been developed. These correspond to adding all higher coherences missing from the 1-categorical formulation. To circumvent the problem of constructing infinite semisimplicial types in standard HoTT, definitions are often presented in Two-Level Type Theory (2LTT), which introduces a "strict equality" (=s) alongside the usual HoTT equality (=) to handle infinite structures.
Despite these challenges, HoTT holds promise for future advancements in formal verification by enabling more powerful verification environments with better support for reusability and modularity, potentially scaling to larger systems. It also contributes to the broader goal of increasing trust in software by establishing machine-checked mathematical proofs guaranteeing correct program behavior.
4.6 Emerging Applications and Future Directions
Hybrid Type Theory, with its ability to bridge formal systems and computational models, is poised to impact several emerging fields, particularly in the realm of artificial intelligence and advanced resource management.
Verified AI and Machine Learning: The increasing complexity and criticality of AI systems, especially machine learning models, necessitate robust verification methods. Dependent types offer a promising avenue for achieving provably correct AI.
Ontologies and Knowledge Graphs: DTT can provide a unified type system that acts as ontology, data schema, and logic simultaneously. This means that type definitions can incorporate actual data or conditions, allowing the type system to enforce properties and integrity constraints automatically. For example, a Person type could enforce that birthDate < deathDate if a deathDate is provided. This shifts consistency checks from runtime to compile-time, replacing separate schema languages like SHACL.
Formal Verification of ML Models: While traditional formal methods struggle with the opaque and adaptive nature of ML models, research is exploring how DTT can contribute to their verification. Dependent types can help specify properties like robustness (e.g., how sensitive a classifier is to small input perturbations) and ensure well-specified systems through constraints. The Curry-Howard correspondence can be leveraged to represent logical propositions about model behavior as types, and proofs of these properties as terms.
Explainable AI (XAI): While not directly using dependent types, the principles of formalization and structured reasoning inherent in type theory could inform the development of more rigorous and verifiable XAI methods. XAI aims to make AI decisions understandable, categorizing methods into intrinsically interpretable models, post-hoc explanations, and visualization approaches. The precision offered by dependent types could contribute to formalizing the "why" behind AI decisions, moving beyond mere approximations.
Advanced Resource Management: The concepts from linear dependent types are crucial for systems requiring fine-grained control over resources. This includes:
Implicit Complexity Analysis: Linear dependent type systems can be used to analyze implicit complexity, allowing resource usage to depend on runtime information like data structure size. This is vital for predicting performance and ensuring efficiency in complex algorithms.
Safe Concurrent and Distributed Systems: By rigorously tracking resource ownership and usage, linear dependent types can help design and verify concurrent and distributed systems that are free from common errors like race conditions, deadlocks, and resource leaks.
Future Directions:
Homotopy Type Theory (HoTT): The ongoing research in HoTT, which combines DTT with homotopy theory, opens avenues for formalizing complex topological structures and higher category theory, potentially revolutionizing mathematical formalization and verification.
Higher-Order Dependent Types: Extending dependent theories to incorporate higher-order types could further enhance their expressivity, allowing for more abstract and powerful specifications.
Integration with Other Formal Methods: Combining DTT with other formal verification techniques, such as model checking and testing, could provide a more comprehensive approach to system assurance.
Tooling and Accessibility: Continued development of user-friendly proof assistants and dependently-typed programming languages will be crucial for broader adoption and impact across various domains.
The future of Hybrid Type Theory is promising, with its potential to provide a unified and rigorous foundation for developing highly reliable and efficient computational systems across an expanding range of applications.
Conclusions
Hybrid Type Theory (HTT) represents a significant advancement in the landscape of formal systems and computational models. This report has demonstrated that HTT is not merely an amalgamation of existing theories but a sophisticated synthesis designed to overcome fundamental expressive limitations inherent in individual formalisms. By integrating dependent types with substructural logics and computational models, HTT provides a powerful framework for simultaneously reasoning about program correctness, resource efficiency, and state management.
The foundational role of Dependent Type Theory (DTT) is paramount, particularly through the lens of the Curry-Howard correspondence. This principle transforms type checking into a rigorous method of proof verification, enabling the construction of provably correct software and machine-checkable mathematical proofs. The nuanced distinction between judgmental and propositional equality within Martin-Löf Type Theory further refines how correctness is established, balancing definitional properties with explicit logical derivations.
The categorical interpretation, notably through Left-Fibred Double Categories (LFDCs), provides a unifying metatheory that reconciles the seemingly conflicting structural rules of dependent types and substructural logics. This abstract framework allows for a precise separation of type-level contemplation from term-level resource consumption, as exemplified by its successful application in representing cut admissibility for linear sequent calculus. This demonstrates HTT's capacity to enforce critical properties like linearity, preventing errors that plague less rigorous systems.
In the practical domain, dependently-typed programming languages such as Idris, Agda, and Coq embody these theoretical advancements, enabling developers to embed complex invariants directly into code, thereby reducing runtime errors and enhancing software reliability. Formal verification, powered by DTT-based proof assistants, has achieved landmark successes in verifying critical software components like the CompCert C compiler and the seL4 microkernel, as well as complex mathematical conjectures.
Challenges such as the undecidability of type checking in DTT are being actively addressed through innovative solutions like stratified type universes (e.g., StraTT, Coq's universe polymorphism), gradual typing, and normalization by evaluation. These approaches aim to balance expressive power with practical usability. Furthermore, the reinterpretation of identity types in Homotopy Type Theory (HoTT) as paths opens new avenues for modeling mathematical equivalences and enhancing modularity in formal verification, despite its current computational complexities.
Looking forward, HTT's principles are increasingly relevant for emerging applications, particularly in the verification of artificial intelligence and machine learning models. Its ability to provide a unified framework for ontologies, data schemas, and logical constraints holds immense potential for building provably correct and trustworthy AI systems. The continuous evolution of HTT promises to deliver more robust, efficient, and verifiable computational solutions across diverse and increasingly complex domains.
Works cited
1. Type theory - Wikipedia, https://en.wikipedia.org/wiki/Type_theory 2. [2401.15258] Foundations of Substructural Dependent Type Theory - arXiv, https://arxiv.org/abs/2401.15258 3. (PDF) Hybrid Partial-Total Type Theory - ResearchGate, https://www.researchgate.net/publication/2739564_Hybrid_Partial-Total_Type_Theory 4. Foundations of Substructural Dependent Type Theory, https://arxiv.org/pdf/2401.15258 5. Dependent type theory and mathematical formalization | Collège de ..., https://www.college-de-france.fr/en/agenda/lecture/dependent-type-theory-and-mathematical-formalization 6. The Evolution of Type Theory - Number Analytics, https://www.numberanalytics.com/blog/evolution-type-theory-logic 7. Intuitionistic Type Theory - Stanford Encyclopedia of Philosophy, https://plato.stanford.edu/entries/type-theory-intuitionistic/ 8. Martin Löf's Type Theory: constructive set theory and logical framework, https://cs-people.bu.edu/gaboardi/publication/ML_types.pdf 9. Mastering Type Theory - Number Analytics, https://www.numberanalytics.com/blog/mastering-type-theory 10. Dependent type - Wikipedia, https://en.wikipedia.org/wiki/Dependent_type 11. Mastering Dependent Types in Proof Theory - Number Analytics, https://www.numberanalytics.com/blog/ultimate-guide-dependent-types-proof-theory 12. A few notes on Idris - nabilhassein.github.io, https://nabilhassein.github.io/blog/notes-on-idris/ 13. CS 4110 – Programming Languages and Logics Lecture #32: Dependent Types 1 Typing Lists with Lengths - CS@Cornell, https://www.cs.cornell.edu/courses/cs4110/2024sp/lectures/lecture32.pdf 14. Example - Idris, https://www.idris-lang.org/pages/example.html 15. Dependent types - Agda's documentation!, https://agda.readthedocs.io/en/v2.6.0.1/getting-started/what-is-agda.html 16. Developing Dependently Typed Programs in Agda - Part 2 - Rodrigo Ribeiro, https://rodrigogribeiro.github.io/posts/2014/07/developing-dependently-typed-programs-agda-2/ 17. 2. Dependent Type Theory — Theorem Proving in Lean 3 (outdated) 3.23.0 documentation, https://leanprover.github.io/theorem_proving_in_lean/dependent_type_theory.html 18. lean4/doc/examples/interp.lean at master - GitHub, https://github.com/leanprover/lean4/blob/master/doc/examples/interp.lean 19. Documentation for the Idris Language - Idris, https://docs.idris-lang.org/_/downloads/en/v1.0/pdf/ 20. Dependent Types in Practical Programming - CMU School of Computer Science, https://www.cs.cmu.edu/~fp/papers/popl99.pdf 21. Examples of Dependent Types - Computer Science Stack Exchange, https://cs.stackexchange.com/questions/56283/examples-of-dependent-types 22. Dependent Types: A New Paradigm for Ontologies and Knowledge Graphs, https://ai.plainenglish.io/dependent-types-a-new-paradigm-for-ontologies-and-knowledge-graphs-199849243bea 23. Provably Correct Software with Dependent Types - Andrea Laretto, https://iwilare.com/provably-correct-software-with-dependent-types.pdf 24. Tensor Flow and Dependent Types - Monday Morning Haskell, https://mmhaskell.com/machine-learning/dependent-types 25. Dependent Type Theory in Practice - Number Analytics, https://www.numberanalytics.com/blog/dependent-type-theory-in-practice 26. Dependent Types in Practice - Number Analytics, https://www.numberanalytics.com/blog/dependent-types-in-practice 27. Mastering Dependent Theories - Number Analytics, https://www.numberanalytics.com/blog/mastering-dependent-theories-advanced-proof-theory 28. In Praise of Dependent Types | The n-Category Café, https://golem.ph.utexas.edu/category/2010/03/in_praise_of_dependent_types.html 29. AgdaTutorial - Martin Escardo, https://martinescardo.github.io/AgdaTutorial/html/AgdaTutorial.html 30. 2. Dependent types — Logic in computer science lecture notes - Andrej Bauer, https://www.andrej.com/zapiski/ISRM-LOGRAC-2022/02-dependent-types.lagda.html 31. Types and Functions — Idris2 0.0 documentation - Read the Docs, https://idris2.readthedocs.io/en/latest/tutorial/typesfuns.html 32. Theorem Proving — Idris2 0.0 documentation - Read the Docs, https://idris2.readthedocs.io/en/latest/tutorial/theorems.html 33. Proof in Idris - Proving Equalities - EuclideanSpace, https://www.euclideanspace.com/maths/proof/idris/proof/index.htm 34. Proof-assistants-using-dependent-type-systems.pdf, https://www.infomath-bib.de/tmp/data/Proof-assistants-using-dependent-type-systems.pdf 35. Dependent Types in Proof Theory - Number Analytics, https://www.numberanalytics.com/blog/dependent-types-in-proof-theory 36. Formalisation of Dependent Type Theory: The Example of CaTT - DROPS, https://drops.dagstuhl.de/storage/00lipics/lipics-vol239-types2021/LIPIcs.TYPES.2021.2/LIPIcs.TYPES.2021.2.pdf 37. Agda: Equality - LMU, Informatik, TCS, https://www2.tcs.ifi.lmu.de/~abel/Equality.pdf 38. Agda: Equality - Page has been moved, https://www.cse.chalmers.se/~abela/Equality.pdf 39. 13. Equality and Induction in Lean — Logic and Mechanized Reasoning 0.1 documentation, https://avigad.github.io/lamr/equality_and_induction_in_lean.html 40. Equality — Formalising Mathematics 0.1 documentation, https://www.ma.imperial.ac.uk/~buzzard/xena/formalising-mathematics-2022/Part_B/equality.html 41. Logic in Coq, https://home.uncg.edu/cmp/faculty/srtate/495.f16/sf/Logic.html 42. 3110 Coq Tactics Cheatsheet - CS@Cornell, https://www.cs.cornell.edu/courses/cs3110/2018sp/a5/coq-tactics-cheatsheet.html 43. A 2-categorical approach to the semantics of dependent type ... - arXiv, https://arxiv.org/abs/2507.07208 44. Internal $\infty $-Categorical Models of Dependent Type Theory ..., https://arxiv.org/abs/2009.01883 45. linear dependent types - pigworker in a space - WordPress.com, https://pigworker.wordpress.com/2015/01/05/linear-dependent-types/ 46. Dependent Type Theory for Programming and Proving* - CMU School of Computer Science, https://www.cs.cmu.edu/~rwh/courses/atpl/pdfs/dependency.pdf 47. Categorical models of subtyping - arXiv, https://arxiv.org/html/2312.14600v1 48. Categorical Models of Dependent Type Theory - Free, http://enslyon.free.fr/rapports/info/Alexandre_Buisse_2.pdf 49. Syntax and Semantics of Dependent Types - Computer Science, https://www.cs.uoregon.edu/research/summerschool/summer14/rwh_notes/ssdt.pdf 50. Applying Type Theory for Robust Formal Verification, https://www.numberanalytics.com/blog/applying-type-theory-for-robust-formal-verification 51. Agda (programming language) - Wikipedia, https://en.wikipedia.org/wiki/Agda_(programming_language) 52. agda/examples/Introduction/Basics.agda at master - GitHub, https://github.com/agda/agda/blob/master/examples/Introduction/Basics.agda 53. More about programming with dependent types. - CS@Cornell, https://www.cs.cornell.edu/courses/cs6115/2017fa/notes/lecture8.html 54. Programming with dependent types: passing fad or useful tool?, https://www.cs.ox.ac.uk/ralf.hinze/WG2.8/26/slides/xavier.pdf 55. DependentTypes - GitHub Pages, https://jackfoxy.github.io/DependentTypes/ 56. What is dependent typing? - functional programming - Stack Overflow, https://stackoverflow.com/questions/9338709/what-is-dependent-typing 57. Dependent types over datatypes - Stack Overflow, https://stackoverflow.com/questions/48534403/dependent-types-over-datatypes 58. Dependent Types in ATS - Bluish Coder, https://bluishcoder.co.nz/2010/09/01/dependent-types-in-ats.html 59. type-theory · GitHub Topics, https://github.com/topics/type-theory 60. vrahli/NuprlInCoq: Implementation of Nuprl's type theory in Coq - GitHub, https://github.com/vrahli/NuprlInCoq 61. Nuprl - Wikipedia, https://en.wikipedia.org/wiki/Nuprl 62. Nuprl - CLiki, https://www.cliki.net/Nuprl 63. nuprl/MultiPL-E: A multi-programming language benchmark for LLMs - GitHub, https://github.com/nuprl/MultiPL-E 64. nuprl/CanItEdit: Can It Edit? Evaluating the Ability of Large Language Models to Follow Code Editing Instructions - GitHub, https://github.com/nuprl/CanItEdit 65. Deducteam/matita_lib_in_agda - GitHub, https://github.com/Deducteam/matita_lib_in_agda 66. Matita - Wikipedia, https://en.wikipedia.org/wiki/Matita 67. Matita Edizioni, https://matitaedizioni.com/en/ 68. Matita | Perfume | PHILIPARTIN'S - Philip Martin's, https://www.philipmartins.it/en/product-page/matita-profumo-corpo 69. sacerdot/matita: An Interactive Theorem Prover for a variant of the Calculus of (Co)Inductive Constructions - GitHub, https://github.com/sacerdot/matita 70. LPCIC/matita: Matita (proof assistant) with embedded elpi - GitHub, https://github.com/LPCIC/matita 71. A Twelf Introduction - C&C, https://jozefg.bitbucket.io/posts/2015-02-28-twelf.html 72. Dependent Types And Safer Code - Paulo Suzart, https://paulosuzart.github.io/blog/2017/06/18/dependent-types-and-safer-code/ 73. jaegertracing/spark-dependencies: Spark job for dependency links - GitHub, https://github.com/jaegertracing/spark-dependencies 74. Apache Spark - A unified analytics engine for large-scale data processing - GitHub, https://github.com/apache/spark 75. Formal Verification of Prim's Algorithm in SPARK - ScholarSpace, https://scholarspace.manoa.hawaii.edu/bitstreams/19b1acf1-eae2-4edd-bfac-07438459f9e1/download 76. 5.5. Specification Features — SPARK User's Guide 26.0w - Documentation - AdaCore, https://docs.adacore.com/spark2014-docs/html/ug/en/source/specification_features.html 77. Coq dependent types - Stack Overflow, https://stackoverflow.com/questions/26090882/coq-dependent-types 78. Type Theory Essentials for Formal Verification - Number Analytics, https://www.numberanalytics.com/blog/type-theory-essentials-for-formal-verification 79. Applying Homotopy Type Theory - Number Analytics, https://www.numberanalytics.com/blog/applying-homotopy-type-theory 80. Applying Dependent Type Theory - Number Analytics, https://www.numberanalytics.com/blog/applying-dependent-type-theory 81. Type-Checking Linear Dependent Types, https://www.irif.fr/~letouzey/types2014/abstract-34.pdf 82. Type-checking - PLS Lab, https://www.pls-lab.org/en/Type_checking 83. Decidable Type-Checking for Bidirectional Martin-Löf Type Theory - Meven Lennon-Bertrand, https://www.meven.ac/documents/23-06-Types-Bidir_Abstract.pdf 84. Undecidability, https://www.cs.rochester.edu/u/nelson/courses/csc_173/computability/undecidable.html 85. TYPECHECKING IS UNDECIDABLE WHEN 'TYPE' IS A TYPE - DSpace@MIT, https://dspace.mit.edu/bitstream/handle/1721.1/149683/MIT-LCS-TR-458.pdf 86. Delving into Ramified Type Theory - Number Analytics, https://www.numberanalytics.com/blog/delving-into-ramified-type-theory 87. The Logic of Self-Reference - Number Analytics, https://www.numberanalytics.com/blog/ultimate-guide-to-self-reference 88. Stratified Type Theory - CIS UPenn - University of Pennsylvania, https://www.cis.upenn.edu/~sweirich/papers/esop25.pdf 89. Universe Polymorphism in Coq, https://www.asc.ohio-state.edu/pollard.4/type/readings/universe_polymorphism.pdf 90. Approximate Normalization for Gradual Dependent Types (ICFP ..., https://icfp19.sigplan.org/details/icfp-2019-papers/22/Approximate-Normalization-for-Gradual-Dependent-Types 91. Partial Gradual Dependent Type Theory (SPLASH 2023 - Student Research Competition), https://2023.splashcon.org/details/splash-2023-SRC/8/Partial-Gradual-Dependent-Type-Theory 92. Checking Dependent Types with Normalization by Evaluation: A Tutorial, https://davidchristiansen.dk/tutorials/nbe/ 93. Practical Dependent Type Checking using Twin Types - Page has been moved, https://www.cse.chalmers.se/~nad/publications/lopez-juan-danielsson-tyde2020.pdf 94. (PDF) Dependent Multiplicities in Dependent Linear Type Theory, https://www.researchgate.net/publication/393656211_Dependent_Multiplicities_in_Dependent_Linear_Type_Theory 95. Homotopy Type Theory: Programming and Verification - UKRI Gateway to Research, https://gtr.ukri.org/projects?ref=EP%2FM016951%2F1 96. 2.3.3 – Verification of the learned model - University of York, https://www.york.ac.uk/media/assuring-autonomy/bodyofknowledgestructure/section2imagesanddocs/2.3.3%20cross%20domain%20and%20automotive%20practical%20guidance%20TIGARS.pdf 97. What are the types of Explainable AI methods? - Milvus, https://milvus.io/ai-quick-reference/what-are-the-types-of-explainable-ai-methods
