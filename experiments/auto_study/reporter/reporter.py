#!/usr/bin/env python3
"""
Auto-Study Reporter Team

Generates human-readable reports from observer and analyst data.
"""

import os
import sys
import json
import logging
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, List, Optional
from jinja2 import Template

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | REPORTER | %(message)s'
)
logger = logging.getLogger(__name__)


REPORT_TEMPLATE = """
# Experiment Report: {{ experiment_name }}

> Generated: {{ timestamp }}
> Status: {{ status }}

---

## Overview

| Metric | Value |
|--------|-------|
| Total Steps | {{ total_steps | default('N/A') }} |
| Duration | {{ duration | default('N/A') }} |
| Progress | {{ progress | default('N/A') }}% |

---

## Decision Analysis

{% if decision_analysis %}
### Distribution

| Decision | Frequency |
|----------|-----------|
{% for decision, freq in decision_analysis.distribution.items() %}
| {{ decision }} | {{ "%.2f"|format(freq * 100) }}% |
{% endfor %}

### Statistics

- **Entropy**: {{ "%.4f"|format(decision_analysis.entropy) }}
- **Confidence Mean**: {{ "%.4f"|format(decision_analysis.confidence_stats.mean) }}
- **Latency Mean**: {{ "%.2f"|format(decision_analysis.latency_stats.mean) }}ms

### Detected Sequences

{% if decision_analysis.sequences %}
{% for seq in decision_analysis.sequences[:5] %}
- `{{ seq.sequence }}` ({{ seq.occurrences }} occurrences)
{% endfor %}
{% else %}
No significant sequences detected.
{% endif %}

{% else %}
No decision data available.
{% endif %}

---

## Anomalies

{% if anomalies %}
| Metric | Value | Expected | Deviation |
|--------|-------|----------|-----------|
{% for a in anomalies[:10] %}
| {{ a.metric }} | {{ "%.4f"|format(a.value) }} | {{ "%.4f"|format(a.mean) }} | {{ "%.2f"|format(a.deviation) }}Ïƒ |
{% endfor %}
{% else %}
No anomalies detected.
{% endif %}

---

## Recommendations

{% if recommendations %}
{% for rec in recommendations %}
- {{ rec }}
{% endfor %}
{% else %}
- Continue monitoring for emergent patterns
- Consider checkpointing more frequently if anomalies increase
{% endif %}

---

## Next Steps

1. Review decision distribution for expected behavior
2. Investigate any anomalies flagged above
3. Compare with baseline (classic Langton's Ant)

---

*Report generated by Auto-Study Reporter Team*
"""


class Reporter:
    """Generates reports from experiment data."""
    
    def __init__(self, experiments_dir: str = None):
        self.experiments_dir = Path(experiments_dir or '/home/adam/worxpace/gladius/experiments')
        self.output_dir = self.experiments_dir / 'auto_study' / 'reporter' / 'reports'
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        self.template = Template(REPORT_TEMPLATE)
    
    def generate_report(self, experiment_name: str, analysis: Dict[str, Any] = None) -> str:
        """Generate a report for an experiment."""
        
        # Load latest analysis if not provided
        if not analysis:
            analysis = self._load_latest_analysis(experiment_name)
        
        if not analysis:
            return f"# No analysis data available for {experiment_name}"
        
        # Build report context
        context = {
            'experiment_name': experiment_name,
            'timestamp': datetime.now().isoformat(),
            'status': analysis.get('status', 'unknown'),
            'decision_analysis': analysis.get('decision_analysis'),
            'anomalies': analysis.get('anomalies', []),
            'recommendations': self._generate_recommendations(analysis)
        }
        
        # Add progress info
        progress = analysis.get('progress', {})
        context['total_steps'] = progress.get('current_step', 'N/A')
        context['progress'] = round(progress.get('estimated_progress', 0), 2)
        
        # Render report
        report = self.template.render(**context)
        
        # Save report
        report_file = self.output_dir / f"{experiment_name}_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
        with open(report_file, 'w') as f:
            f.write(report)
        
        logger.info(f"Report saved: {report_file}")
        
        return report
    
    def _load_latest_analysis(self, experiment_name: str) -> Optional[Dict[str, Any]]:
        """Load the latest analysis for an experiment."""
        analyst_dir = self.experiments_dir / 'auto_study' / 'analyst' / 'analysis'
        
        analysis_files = sorted(
            analyst_dir.glob(f"{experiment_name}_analysis_*.json"),
            reverse=True
        )
        
        if analysis_files:
            with open(analysis_files[0]) as f:
                return json.load(f)
        
        return None
    
    def _generate_recommendations(self, analysis: Dict[str, Any]) -> List[str]:
        """Generate recommendations based on analysis."""
        recommendations = []
        
        decision_analysis = analysis.get('decision_analysis', {})
        anomalies = analysis.get('anomalies', [])
        
        # Check entropy
        entropy = decision_analysis.get('entropy', 0)
        if entropy < 0.5:
            recommendations.append("Low entropy detected - decisions may be too predictable")
        elif entropy > 1.5:
            recommendations.append("High entropy - decisions appear random, may indicate model issues")
        
        # Check confidence
        conf_stats = decision_analysis.get('confidence_stats', {})
        if conf_stats.get('mean', 1) < 0.7:
            recommendations.append("Low average confidence - model may need more training")
        
        # Check latency
        latency_stats = decision_analysis.get('latency_stats', {})
        if latency_stats.get('p95', 0) > 100:
            recommendations.append("High latency (>100ms) - consider optimizing inference")
        
        # Check anomalies
        if len(anomalies) > 5:
            recommendations.append(f"Multiple anomalies detected ({len(anomalies)}) - investigate data quality")
        
        return recommendations
    
    def generate_summary(self) -> str:
        """Generate a summary report of all experiments."""
        summary_lines = [
            "# Experiments Summary",
            f"\n> Generated: {datetime.now().isoformat()}\n",
            "---\n",
            "| Experiment | Status | Steps | Anomalies |",
            "|------------|--------|-------|-----------|"
        ]
        
        for item in self.experiments_dir.iterdir():
            if item.is_dir() and item.name not in ['auto_study', 'templates', '__pycache__']:
                analysis = self._load_latest_analysis(item.name)
                if analysis:
                    status = analysis.get('status', 'unknown')
                    steps = analysis.get('progress', {}).get('current_step', 'N/A')
                    anomalies = len(analysis.get('anomalies', []))
                    summary_lines.append(f"| {item.name} | {status} | {steps} | {anomalies} |")
        
        summary = '\n'.join(summary_lines)
        
        # Save summary
        summary_file = self.output_dir / f"summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
        with open(summary_file, 'w') as f:
            f.write(summary)
        
        return summary


def main():
    """Main entry point."""
    import argparse
    
    parser = argparse.ArgumentParser(description='Auto-Study Reporter Team')
    parser.add_argument('--experiment', type=str, help='Specific experiment to report')
    parser.add_argument('--summary', action='store_true', help='Generate summary of all experiments')
    
    args = parser.parse_args()
    
    reporter = Reporter()
    
    if args.summary:
        summary = reporter.generate_summary()
        print(summary)
    elif args.experiment:
        report = reporter.generate_report(args.experiment)
        print(report)
    else:
        # Generate all reports
        for item in reporter.experiments_dir.iterdir():
            if item.is_dir() and item.name not in ['auto_study', 'templates', '__pycache__']:
                reporter.generate_report(item.name)
        
        reporter.generate_summary()


if __name__ == '__main__':
    main()
