#!/usr/bin/env python3
"""
GLADIUS Comprehensive System Test Suite
========================================

Tests all critical aspects of the GLADIUS system:
1. Tool Discovery and Execution
2. Discovery Mechanisms (SENTINEL)
3. Inference Capabilities
4. Workspace Control
5. System Integration

This validates that GLADIUS has complete control and intelligence.

Author: Artifact Virtual Systems
"""

import os
import sys
import json
import asyncio
import sqlite3
import tempfile
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, List, Optional

# Setup paths
GLADIUS_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(GLADIUS_ROOT))
sys.path.insert(0, str(GLADIUS_ROOT / "GLADIUS"))
sys.path.insert(0, str(GLADIUS_ROOT / "SENTINEL"))
sys.path.insert(0, str(GLADIUS_ROOT / "LEGION" / "legion"))

# Test results tracking
results = {
    "passed": [],
    "failed": [],
    "warnings": [],
    "start_time": datetime.now().isoformat(),
}


# ============================================================================
# Test Utilities
# ============================================================================

def test_pass(category: str, name: str, msg: str = ""):
    """Mark a test as passed"""
    results["passed"].append({"category": category, "name": name, "msg": msg})
    print(f"  ‚úÖ [{category}] {name}: {msg}")


def test_fail(category: str, name: str, msg: str = ""):
    """Mark a test as failed"""
    results["failed"].append({"category": category, "name": name, "msg": msg})
    print(f"  ‚ùå [{category}] {name}: {msg}")


def test_warn(category: str, name: str, msg: str = ""):
    """Mark a test as warning"""
    results["warnings"].append({"category": category, "name": name, "msg": msg})
    print(f"  ‚ö†Ô∏è  [{category}] {name}: {msg}")


# ============================================================================
# PHASE 1: Tool Discovery and Execution
# ============================================================================

async def test_tool_discovery():
    """Test that GLADIUS can discover and list available tools"""
    print("\nüì¶ Phase 1: Tool Discovery")
    print("=" * 80)
    
    try:
        from GLADIUS.router.pattern_router import NativeToolRouter
        
        router = NativeToolRouter()
        
        # Test router initialization
        if router:
            test_pass("Tools", "Router Init", "Router initialized successfully")
        else:
            test_fail("Tools", "Router Init", "Router failed to initialize")
            return
        
        # Test tool pattern availability
        if hasattr(router, 'TOOL_PATTERNS') and len(router.TOOL_PATTERNS) > 0:
            tool_count = len(router.TOOL_PATTERNS)
            test_pass("Tools", "Pattern Discovery", f"Found {tool_count} tool patterns")
        else:
            test_fail("Tools", "Pattern Discovery", "No tool patterns found")
        
        # Test basic tool patterns exist
        expected_tools = ["search", "read_file", "list_dir", "remember", "recall", "get_context"]
        for tool in expected_tools:
            if tool in router.TOOL_PATTERNS:
                test_pass("Tools", f"Tool: {tool}", "Pattern available")
            else:
                test_warn("Tools", f"Tool: {tool}", "Pattern not found")
        
    except Exception as e:
        test_fail("Tools", "Discovery Error", str(e))


async def test_tool_routing():
    """Test that GLADIUS can route queries to appropriate tools"""
    print("\nüîÄ Tool Routing Tests")
    print("-" * 80)
    
    try:
        from GLADIUS.router.pattern_router import NativeToolRouter
        
        router = NativeToolRouter()
        
        # Test cases: query -> expected tool
        test_cases = [
            ("search for gold prices", "search"),
            ("read file config.json", "read_file"),
            ("list files in /tmp", "list_dir"),
            ("remember that the API key is xyz", "remember"),
            ("recall what you know about gold", "recall"),
        ]
        
        for query, expected_tool in test_cases:
            try:
                result = router.route(query)
                
                if result.success and result.tool_name:
                    actual_tool = result.tool_name
                    if actual_tool == expected_tool:
                        test_pass("Routing", f"Query: '{query[:30]}...'", 
                                f"Routed to {actual_tool} ({result.latency_ms:.2f}ms)")
                    else:
                        test_warn("Routing", f"Query: '{query[:30]}...'", 
                                f"Expected {expected_tool}, got {actual_tool}")
                else:
                    test_fail("Routing", f"Query: '{query[:30]}...'", 
                            f"Failed: {result.error}")
                    
            except Exception as e:
                test_fail("Routing", f"Query: '{query[:30]}...'", str(e))
                
    except Exception as e:
        test_fail("Routing", "Setup Error", str(e))


async def test_tool_execution():
    """Test that tools can be executed with proper arguments"""
    print("\n‚öôÔ∏è  Tool Execution Tests")
    print("-" * 80)
    
    # Test workspace file operations
    temp_dir = Path(tempfile.mkdtemp())
    try:
        test_file = temp_dir / "test.txt"
        test_content = "GLADIUS test content"
        
        # Write test
        test_file.write_text(test_content)
        if test_file.exists():
            test_pass("Execution", "Write File", f"Created {test_file.name}")
        else:
            test_fail("Execution", "Write File", "File creation failed")
        
        # Read test
        content = test_file.read_text()
        if content == test_content:
            test_pass("Execution", "Read File", "Content matches")
        else:
            test_fail("Execution", "Read File", "Content mismatch")
        
        # List directory test
        files = list(temp_dir.glob("*"))
        if len(files) == 1 and files[0].name == "test.txt":
            test_pass("Execution", "List Directory", f"Found {len(files)} file(s)")
        else:
            test_fail("Execution", "List Directory", "Unexpected directory content")
        
        # Cleanup
        test_file.unlink()
        temp_dir.rmdir()
        test_pass("Execution", "Cleanup", "Temporary files removed")
        
    except Exception as e:
        test_fail("Execution", "File Operations", str(e))
    finally:
        # Ensure cleanup even if exception occurs
        import shutil
        if temp_dir.exists():
            shutil.rmtree(temp_dir)


# ============================================================================
# PHASE 2: Discovery Mechanisms (SENTINEL)
# ============================================================================

async def test_discovery_mechanisms():
    """Test SENTINEL's discovery and research capabilities"""
    print("\nüîç Phase 2: Discovery Mechanisms")
    print("=" * 80)
    
    try:
        from SENTINEL.services.learning_daemon import WebResearcher
        
        researcher = WebResearcher()
        
        # Test researcher initialization
        test_pass("Discovery", "Researcher Init", "WebResearcher initialized")
        
        # Test keyword extraction capability
        test_keywords = ["machine learning", "AI", "neural networks"]
        if researcher:
            test_pass("Discovery", "Keywords", f"Ready to search {len(test_keywords)} topics")
        
        # Test arXiv search capability (without actually calling API to avoid rate limits)
        if hasattr(researcher, 'search_arxiv'):
            test_pass("Discovery", "arXiv Integration", "Method available")
        else:
            test_warn("Discovery", "arXiv Integration", "Method not found")
        
        # Test GitHub search capability
        if hasattr(researcher, 'search_github'):
            test_pass("Discovery", "GitHub Integration", "Method available")
        else:
            test_warn("Discovery", "GitHub Integration", "Method not found")
        
        # Cleanup
        await researcher.close()
        test_pass("Discovery", "Cleanup", "Researcher closed")
        
    except Exception as e:
        test_fail("Discovery", "Setup Error", str(e))


async def test_learning_loop():
    """Test SENTINEL's learning loop functionality"""
    print("\nüîÑ Learning Loop Tests")
    print("-" * 80)
    
    try:
        from SENTINEL.services.learning_daemon import LearningDaemon, LearningPhase
        
        daemon = LearningDaemon()
        
        # Test daemon initialization
        test_pass("Learning", "Daemon Init", "LearningDaemon initialized")
        
        # Test state database (using Artifact adapter)
        daemon._init_state_db()
        if daemon.artifact_db:
            test_pass("Learning", "State Database", "Artifact DB adapter available")
        else:
            test_warn("Learning", "State Database", "Using fallback storage")
        
        # Test state loading
        state = daemon._load_state()
        if state:
            test_pass("Learning", "State Load", f"Phase: {state.current_phase.value}")
        else:
            test_fail("Learning", "State Load", "State not loaded")
        
        # Test checkpoint system
        daemon.state = state
        original_cycles = daemon.state.cycles_completed
        daemon.state.cycles_completed = original_cycles + 1
        daemon._save_checkpoint()
        
        # Verify checkpoint by loading state again
        state2 = daemon._load_state()
        if state2.cycles_completed >= original_cycles:
            test_pass("Learning", "Checkpoint Save", f"Cycle {state2.cycles_completed} persisted")
        else:
            test_warn("Learning", "Checkpoint Save", "Checkpoint may not persist without Artifact DB")
        
        # Cleanup
        await daemon.researcher.close()
        
    except Exception as e:
        test_fail("Learning", "Loop Error", str(e))


async def test_knowledge_storage():
    """Test that GLADIUS can store and retrieve knowledge"""
    print("\nüß† Knowledge Storage Tests")
    print("-" * 80)
    
    try:
        from SENTINEL.services.learning_daemon import LearningDaemon
        
        daemon = LearningDaemon()
        daemon._init_state_db()
        
        # Test using Artifact DB or fallback
        if daemon.artifact_db:
            test_pass("Knowledge", "Storage Backend", "Artifact DB available")
            
            # Test that we can store and retrieve state
            state = daemon._load_state()
            if state:
                test_pass("Knowledge", "Load State", "State retrieved successfully")
            else:
                test_fail("Knowledge", "Load State", "Failed to load state")
            
            # Test checkpoint save/load
            original_cycles = state.cycles_completed
            state.cycles_completed += 1
            daemon.state = state
            daemon._save_checkpoint()
            
            # Reload state
            state2 = daemon._load_state()
            if state2.cycles_completed >= original_cycles:
                test_pass("Knowledge", "Store & Retrieve", "Data persisted successfully")
            else:
                test_warn("Knowledge", "Store & Retrieve", "Data persistence uncertain")
        else:
            test_warn("Knowledge", "Storage Backend", "No Artifact DB adapter - using in-memory storage")
        
        await daemon.researcher.close()
        
    except Exception as e:
        test_fail("Knowledge", "Storage Error", str(e))


# ============================================================================
# PHASE 3: Inference Capabilities
# ============================================================================

async def test_inference_capabilities():
    """Test GLADIUS's inference and understanding"""
    print("\nü§ñ Phase 3: Inference Capabilities")
    print("=" * 80)
    
    try:
        from GLADIUS.router.pattern_router import NativeToolRouter
        
        router = NativeToolRouter()
        
        # Test understanding of complex queries
        complex_queries = [
            "I need to find information about gold prices and save it for later",
            "Search the database for recent transactions and remember the results",
            "Look up the file config.json and tell me what's in the data directory",
        ]
        
        for query in complex_queries:
            result = router.route(query)
            if result.success:
                test_pass("Inference", "Complex Query", 
                         f"'{query[:40]}...' -> {result.tool_name} ({result.confidence:.2f} confidence)")
            else:
                test_warn("Inference", "Complex Query", f"Failed to parse: {query[:40]}...")
        
        # Test confidence scoring
        simple_query = "search for gold"
        result = router.route(simple_query)
        if result.confidence >= 0.7:
            test_pass("Inference", "Confidence High", f"{result.confidence:.2f} for simple query")
        elif result.confidence >= 0.5:
            test_warn("Inference", "Confidence Medium", f"{result.confidence:.2f} for simple query")
        else:
            test_fail("Inference", "Confidence Low", f"{result.confidence:.2f} for simple query")
        
        # Test response time
        if result.latency_ms < 100:
            test_pass("Inference", "Response Speed", f"{result.latency_ms:.2f}ms")
        else:
            test_warn("Inference", "Response Speed", f"{result.latency_ms:.2f}ms (>100ms)")
        
    except Exception as e:
        test_fail("Inference", "Setup Error", str(e))


async def test_context_awareness():
    """Test that GLADIUS maintains context and awareness"""
    print("\nüìù Context Awareness Tests")
    print("-" * 80)
    
    try:
        from GLADIUS.router.pattern_router import NativeToolRouter
        
        router = NativeToolRouter()
        
        # Test that router maintains statistics
        stats = router.stats()
        
        if stats:
            test_pass("Context", "Statistics", f"Tracking {stats.get('total_calls', 0)} calls")
        else:
            test_warn("Context", "Statistics", "Stats not available")
        
        # Test fallback mechanisms
        if router.use_ollama:
            test_pass("Context", "Ollama Fallback", "Enabled")
        else:
            test_warn("Context", "Ollama Fallback", "Disabled")
        
        if router.use_pattern:
            test_pass("Context", "Pattern Fallback", "Enabled")
        else:
            test_warn("Context", "Pattern Fallback", "Disabled")
        
    except Exception as e:
        test_fail("Context", "Awareness Error", str(e))


async def test_actionability():
    """Test that GLADIUS responses are actionable"""
    print("\n‚ö° Actionability Tests")
    print("-" * 80)
    
    try:
        from GLADIUS.router.pattern_router import NativeToolRouter
        
        router = NativeToolRouter()
        
        # Test that routing results include necessary information
        result = router.route("search for gold prices")
        
        if result.tool_name:
            test_pass("Actionability", "Tool Name", f"Provided: {result.tool_name}")
        else:
            test_fail("Actionability", "Tool Name", "Not provided")
        
        if result.arguments is not None:
            test_pass("Actionability", "Arguments", f"Provided: {len(result.arguments)} args")
        else:
            test_warn("Actionability", "Arguments", "None provided")
        
        if result.source:
            test_pass("Actionability", "Source", f"Routing source: {result.source}")
        else:
            test_warn("Actionability", "Source", "Not specified")
        
    except Exception as e:
        test_fail("Actionability", "Test Error", str(e))


# ============================================================================
# PHASE 4: Workspace Control
# ============================================================================

async def test_workspace_control():
    """Test GLADIUS's control over the workspace"""
    print("\nüíæ Phase 4: Workspace Control")
    print("=" * 80)
    
    # Create test workspace
    test_workspace = Path(tempfile.mkdtemp())
    
    try:
        # Test directory creation
        sub_dir = test_workspace / "data"
        sub_dir.mkdir()
        if sub_dir.exists():
            test_pass("Workspace", "Create Directory", f"Created {sub_dir.name}")
        else:
            test_fail("Workspace", "Create Directory", "Failed")
        
        # Test file creation
        test_file = sub_dir / "test.json"
        test_data = {"gladius": "test", "timestamp": datetime.now().isoformat()}
        test_file.write_text(json.dumps(test_data, indent=2))
        
        if test_file.exists():
            test_pass("Workspace", "Create File", f"Created {test_file.name}")
        else:
            test_fail("Workspace", "Create File", "Failed")
        
        # Test file reading
        loaded_data = json.loads(test_file.read_text())
        if loaded_data.get("gladius") == "test":
            test_pass("Workspace", "Read File", "Data verified")
        else:
            test_fail("Workspace", "Read File", "Data mismatch")
        
        # Test directory listing
        files = list(sub_dir.glob("*.json"))
        if len(files) == 1:
            test_pass("Workspace", "List Files", f"Found {len(files)} file(s)")
        else:
            test_fail("Workspace", "List Files", f"Unexpected count: {len(files)}")
        
        # Test file modification
        test_data["modified"] = True
        test_file.write_text(json.dumps(test_data, indent=2))
        modified_data = json.loads(test_file.read_text())
        
        if modified_data.get("modified"):
            test_pass("Workspace", "Modify File", "Modification successful")
        else:
            test_fail("Workspace", "Modify File", "Modification failed")
        
        # Test file deletion
        test_file.unlink()
        if not test_file.exists():
            test_pass("Workspace", "Delete File", "Deletion successful")
        else:
            test_fail("Workspace", "Delete File", "File still exists")
        
    except Exception as e:
        test_fail("Workspace", "Control Error", str(e))
    finally:
        # Cleanup
        import shutil
        shutil.rmtree(test_workspace)
        test_pass("Workspace", "Cleanup", "Test workspace removed")


async def test_database_operations():
    """Test GLADIUS's database control"""
    print("\nüóÑÔ∏è  Database Operations Tests")
    print("-" * 80)
    
    # Create test database using secure temp file
    fd, test_db_path = tempfile.mkstemp(suffix=".db")
    test_db = Path(test_db_path)
    
    try:
        os.close(fd)  # Close the file descriptor
        
        conn = sqlite3.connect(test_db)
        cursor = conn.cursor()
        
        # Create test table
        cursor.execute("""
            CREATE TABLE test_data (
                id INTEGER PRIMARY KEY,
                name TEXT,
                value REAL,
                timestamp TEXT
            )
        """)
        test_pass("Database", "Create Table", "Table created")
        
        # Insert data
        cursor.execute("""
            INSERT INTO test_data (name, value, timestamp)
            VALUES (?, ?, ?)
        """, ("gladius_test", 42.0, datetime.now().isoformat()))
        conn.commit()
        test_pass("Database", "Insert Data", "Record inserted")
        
        # Query data
        cursor.execute("SELECT * FROM test_data WHERE name = ?", ("gladius_test",))
        result = cursor.fetchone()
        
        if result and result[1] == "gladius_test" and result[2] == 42.0:
            test_pass("Database", "Query Data", "Record retrieved successfully")
        else:
            test_fail("Database", "Query Data", "Query failed")
        
        # Update data
        cursor.execute("""
            UPDATE test_data SET value = ? WHERE name = ?
        """, (100.0, "gladius_test"))
        conn.commit()
        
        cursor.execute("SELECT value FROM test_data WHERE name = ?", ("gladius_test",))
        updated_value = cursor.fetchone()[0]
        
        if updated_value == 100.0:
            test_pass("Database", "Update Data", "Record updated")
        else:
            test_fail("Database", "Update Data", "Update failed")
        
        # Delete data
        cursor.execute("DELETE FROM test_data WHERE name = ?", ("gladius_test",))
        conn.commit()
        
        cursor.execute("SELECT COUNT(*) FROM test_data WHERE name = ?", ("gladius_test",))
        count = cursor.fetchone()[0]
        
        if count == 0:
            test_pass("Database", "Delete Data", "Record deleted")
        else:
            test_fail("Database", "Delete Data", "Delete failed")
        
        conn.close()
        test_db.unlink()
        test_pass("Database", "Cleanup", "Test database removed")
        
    except Exception as e:
        test_fail("Database", "Operations Error", str(e))
    finally:
        # Ensure cleanup even if exception occurs
        if test_db.exists():
            test_db.unlink()


async def test_memory_management():
    """Test GLADIUS's memory and state management"""
    print("\nüß© Memory Management Tests")
    print("-" * 80)
    
    try:
        from SENTINEL.services.learning_daemon import LearningDaemon
        
        daemon = LearningDaemon()
        daemon._init_state_db()
        
        # Test state persistence
        state = daemon._load_state()
        original_cycles = state.cycles_completed
        
        state.cycles_completed += 1
        daemon.state = state
        daemon._save_checkpoint()
        
        # Load in new instance
        daemon2 = LearningDaemon()
        daemon2._init_state_db()
        state2 = daemon2._load_state()
        
        if state2.cycles_completed >= original_cycles:
            test_pass("Memory", "State Persistence", "State persisted across instances")
        else:
            test_fail("Memory", "State Persistence", "State not persisted")
        
        # Test memory system availability
        if daemon.artifact_db:
            test_pass("Memory", "Storage Backend", "Artifact DB adapter available")
        else:
            test_warn("Memory", "Storage Backend", "Using in-memory storage")
        
        # Test state structure
        if hasattr(state, 'current_phase') and hasattr(state, 'cycles_completed'):
            test_pass("Memory", "State Structure", "All required fields present")
        else:
            test_fail("Memory", "State Structure", "Missing required fields")
        
        await daemon.researcher.close()
        await daemon2.researcher.close()
        
    except Exception as e:
        test_fail("Memory", "Management Error", str(e))


# ============================================================================
# PHASE 5: Integration Testing
# ============================================================================

async def test_legion_integration():
    """Test LEGION-GLADIUS integration"""
    print("\nü§ù Phase 5: Integration Testing")
    print("=" * 80)
    
    try:
        from artifact_bridge import ArtifactBridge
        
        bridge = ArtifactBridge()
        
        # Test bridge initialization
        test_pass("Integration", "Bridge Init", "ArtifactBridge initialized")
        
        # Test integration discovery
        integrations = bridge.available_integrations
        if integrations:
            test_pass("Integration", "Discovery", f"Found {len(integrations)} integrations")
            
            # List specific integrations
            for name, available in integrations.items():
                if available:
                    test_pass("Integration", f"Platform: {name}", "Available")
                else:
                    test_warn("Integration", f"Platform: {name}", "Not available")
        else:
            test_fail("Integration", "Discovery", "No integrations found")
        
        # Test GLADIUS router integration
        if bridge.gladius_router:
            test_pass("Integration", "GLADIUS Router", "Connected")
        else:
            test_warn("Integration", "GLADIUS Router", "Not connected")
        
    except Exception as e:
        test_fail("Integration", "Setup Error", str(e))


async def test_end_to_end_workflow():
    """Test a complete end-to-end workflow"""
    print("\nüîÑ End-to-End Workflow Test")
    print("-" * 80)
    
    try:
        from GLADIUS.router.pattern_router import NativeToolRouter
        
        router = NativeToolRouter()
        
        # Simulate a complete workflow:
        # 1. User query -> 2. Tool routing -> 3. Execution simulation
        
        user_query = "search for gold market analysis"
        
        # Step 1: Parse query
        result = router.route(user_query)
        if result.success:
            test_pass("Workflow", "Query Parsing", f"Parsed: '{user_query}'")
        else:
            test_fail("Workflow", "Query Parsing", f"Failed: {result.error}")
            return
        
        # Step 2: Validate routing
        if result.tool_name == "search":
            test_pass("Workflow", "Tool Selection", f"Selected: {result.tool_name}")
        else:
            test_warn("Workflow", "Tool Selection", f"Unexpected tool: {result.tool_name}")
        
        # Step 3: Validate arguments
        if result.arguments:
            test_pass("Workflow", "Argument Extraction", f"Args: {result.arguments}")
        else:
            test_warn("Workflow", "Argument Extraction", "No arguments extracted")
        
        # Step 4: Simulate execution
        execution_success = True  # Simulated
        if execution_success:
            test_pass("Workflow", "Tool Execution", "Execution simulated successfully")
        else:
            test_fail("Workflow", "Tool Execution", "Execution failed")
        
        # Step 5: Validate response
        if result.latency_ms < 100:
            test_pass("Workflow", "Response Time", f"{result.latency_ms:.2f}ms")
        else:
            test_warn("Workflow", "Response Time", f"{result.latency_ms:.2f}ms (slow)")
        
    except Exception as e:
        test_fail("Workflow", "E2E Error", str(e))


async def test_error_recovery():
    """Test error handling and recovery"""
    print("\nüõ°Ô∏è  Error Recovery Tests")
    print("-" * 80)
    
    try:
        from GLADIUS.router.pattern_router import NativeToolRouter
        
        router = NativeToolRouter()
        
        # Test invalid query handling
        invalid_query = ""
        result = router.route(invalid_query)
        
        if not result.success or result.tool_name == "unknown":
            test_pass("Recovery", "Invalid Query", "Handled gracefully")
        else:
            test_warn("Recovery", "Invalid Query", "Should have failed")
        
        # Test nonsense query
        nonsense_query = "asdfghjkl qwertyuiop"
        result = router.route(nonsense_query)
        
        if result.tool_name in ["unknown", "search"]:  # fallback behavior
            test_pass("Recovery", "Nonsense Query", "Fallback activated")
        else:
            test_warn("Recovery", "Nonsense Query", f"Unexpected: {result.tool_name}")
        
        # Test that router still works after errors
        valid_query = "search for test"
        result = router.route(valid_query)
        
        if result.success and result.tool_name == "search":
            test_pass("Recovery", "Post-Error Recovery", "Router still functional")
        else:
            test_fail("Recovery", "Post-Error Recovery", "Router damaged by errors")
        
    except Exception as e:
        test_fail("Recovery", "Test Error", str(e))


async def test_continuous_operation():
    """Test system can operate continuously"""
    print("\n‚ôæÔ∏è  Continuous Operation Test")
    print("-" * 80)
    
    try:
        from GLADIUS.router.pattern_router import NativeToolRouter
        
        router = NativeToolRouter()
        
        # Simulate multiple sequential operations
        operations = [
            "search for gold",
            "read file test.txt",
            "list files in directory",
            "remember this information",
            "recall previous data",
        ]
        
        success_count = 0
        for i, query in enumerate(operations, 1):
            result = router.route(query)
            if result.success:
                success_count += 1
        
        success_rate = (success_count / len(operations)) * 100
        
        if success_rate >= 80:
            test_pass("Continuous", "Sequential Operations", 
                     f"{success_count}/{len(operations)} successful ({success_rate:.1f}%)")
        elif success_rate >= 60:
            test_warn("Continuous", "Sequential Operations", 
                     f"{success_count}/{len(operations)} successful ({success_rate:.1f}%)")
        else:
            test_fail("Continuous", "Sequential Operations", 
                     f"Only {success_count}/{len(operations)} successful ({success_rate:.1f}%)")
        
        # Test that stats are tracking
        stats = router.stats()
        if stats.get("total_calls", 0) >= len(operations):
            test_pass("Continuous", "Call Tracking", f"{stats.get('total_calls')} calls tracked")
        else:
            test_warn("Continuous", "Call Tracking", "Call count mismatch")
        
    except Exception as e:
        test_fail("Continuous", "Operation Error", str(e))


# ============================================================================
# Main Test Runner
# ============================================================================

async def main():
    """Run all test suites"""
    print("‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó")
    print("‚ïë          GLADIUS COMPREHENSIVE SYSTEM TEST SUITE                               ‚ïë")
    print("‚ïë                                                                                ‚ïë")
    print("‚ïë  Testing: Tools, Discovery, Inference, Workspace Control, Integration         ‚ïë")
    print("‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù")
    print()
    
    # Load environment
    try:
        from dotenv import load_dotenv
        load_dotenv(GLADIUS_ROOT / ".env")
        print("‚úì Environment loaded")
    except:
        print("‚ö† Could not load .env file")
    
    print()
    
    # Run all test phases
    await test_tool_discovery()
    await test_tool_routing()
    await test_tool_execution()
    
    await test_discovery_mechanisms()
    await test_learning_loop()
    await test_knowledge_storage()
    
    await test_inference_capabilities()
    await test_context_awareness()
    await test_actionability()
    
    await test_workspace_control()
    await test_database_operations()
    await test_memory_management()
    
    await test_legion_integration()
    await test_end_to_end_workflow()
    await test_error_recovery()
    await test_continuous_operation()
    
    # Generate summary
    results["end_time"] = datetime.now().isoformat()
    
    print("\n" + "=" * 80)
    print("                              TEST SUMMARY")
    print("=" * 80)
    print(f"  ‚úÖ Passed:   {len(results['passed'])}")
    print(f"  ‚ö†Ô∏è  Warnings: {len(results['warnings'])}")
    print(f"  ‚ùå Failed:   {len(results['failed'])}")
    print("=" * 80)
    
    # Detailed results by category
    categories = {}
    for test in results["passed"]:
        cat = test["category"]
        categories[cat] = categories.get(cat, {"passed": 0, "failed": 0, "warnings": 0})
        categories[cat]["passed"] += 1
    
    for test in results["failed"]:
        cat = test["category"]
        categories[cat] = categories.get(cat, {"passed": 0, "failed": 0, "warnings": 0})
        categories[cat]["failed"] += 1
    
    for test in results["warnings"]:
        cat = test["category"]
        categories[cat] = categories.get(cat, {"passed": 0, "failed": 0, "warnings": 0})
        categories[cat]["warnings"] += 1
    
    print("\nResults by Category:")
    print("-" * 80)
    for cat, counts in sorted(categories.items()):
        total = counts["passed"] + counts["failed"] + counts["warnings"]
        pass_rate = (counts["passed"] / total * 100) if total > 0 else 0
        print(f"  {cat:15s}: {counts['passed']:2d} passed, {counts['failed']:2d} failed, "
              f"{counts['warnings']:2d} warnings ({pass_rate:.1f}% pass rate)")
    
    print("=" * 80)
    
    # Save results to file
    results_file = GLADIUS_ROOT / "tests" / "test_results.json"
    results_file.parent.mkdir(exist_ok=True)
    results_file.write_text(json.dumps(results, indent=2))
    print(f"\nüìÑ Results saved to: {results_file}")
    
    # Determine exit code
    if results['failed']:
        print("\n‚ùå SOME TESTS FAILED")
        print("\nFailed Tests:")
        for test in results['failed']:
            print(f"  - [{test['category']}] {test['name']}: {test['msg']}")
        print()
        return 1
    elif results['warnings']:
        print("\n‚ö†Ô∏è  ALL TESTS PASSED WITH WARNINGS")
        return 0
    else:
        print("\n‚úÖ ALL TESTS PASSED - GLADIUS IS FULLY OPERATIONAL")
        print("\nüéØ System Validation Complete:")
        print("   ‚úì Tools: Discovered, routed, and executable")
        print("   ‚úì Discovery: Research and learning mechanisms operational")
        print("   ‚úì Inference: Understanding, actionability, and context awareness verified")
        print("   ‚úì Workspace: Full control over files, databases, and memory")
        print("   ‚úì Integration: All subsystems connected and functional")
        return 0


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)
